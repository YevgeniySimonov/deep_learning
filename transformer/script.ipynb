{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 690/690 [00:00<00:00, 298kB/s]\n",
      "Downloading: 100%|██████████| 4.03k/4.03k [00:00<00:00, 1.52MB/s]\n",
      "Downloading: 100%|██████████| 525k/525k [00:01<00:00, 305kB/s]  \n",
      "Downloading: 100%|██████████| 316/316 [00:00<00:00, 102kB/s]\n",
      "Downloading: 100%|██████████| 605M/605M [01:02<00:00, 9.76MB/s] \n",
      "Downloading: 100%|██████████| 389/389 [00:00<00:00, 137kB/s]\n",
      "Downloading: 100%|██████████| 604/604 [00:00<00:00, 217kB/s]\n",
      "Downloading: 100%|██████████| 961k/961k [00:02<00:00, 376kB/s]  \n",
      "Downloading: 100%|██████████| 1.88k/1.88k [00:00<00:00, 2.34MB/s]\n",
      "Downloading: 100%|██████████| 116/116 [00:00<00:00, 89.6kB/s]\n",
      "Downloading: 100%|██████████| 122/122 [00:00<00:00, 141kB/s]\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "import glob\n",
    "import torch\n",
    "import pickle\n",
    "import zipfile\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPImage\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# load the CLIP model\n",
    "model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.92G/1.92G [05:06<00:00, 6.27MB/s]  \n",
      "Extracting: 100%|██████████| 24996/24996 [00:08<00:00, 2881.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# get 25k images from Unsplash\n",
    "img_folder = 'photos/'\n",
    "if not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n",
    "    os.makedirs(img_folder, exist_ok=True)\n",
    "\n",
    "    photo_filename = 'unsplash-25k-photos.zip'\n",
    "    if not os.path.exists(photo_filename):  # Download dataset if it doesn't exist\n",
    "        util.http_get('http://sbert.net/datasets/' + photo_filename, photo_filename)\n",
    "\n",
    "    # extract images\n",
    "    with zipfile.ZipFile(photo_filename, 'r') as zf:\n",
    "        for member in tqdm(zf.infolist(), desc='Extracting'):\n",
    "            zf.extract(member, img_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51.8M/51.8M [00:37<00:00, 1.37MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(img_names):  24996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compute the embeddings\n",
    "# distribute pre-computed embeddings\n",
    "# otherwise, can also encode images yourself\n",
    "# to encode the image:\n",
    "# from PIL import Image\n",
    "# img_emb = model.encode(Image.open(filepath))\n",
    "\n",
    "use_precomputed_embeddings = True\n",
    "\n",
    "if use_precomputed_embeddings:\n",
    "    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n",
    "    if not os.path.exists(emb_filename):    # download dataset if it doesn't exist\n",
    "        util.http_get('http://sbert.net/datasets/' + emb_filename, emb_filename)\n",
    "\n",
    "    with open(emb_filename, 'rb') as fIn:\n",
    "        img_names, img_emb = pickle.load(fIn)\n",
    "    print(\"len(img_names): \", len(img_names))\n",
    "else:\n",
    "    img_names = list(glob.glob('unsplash/photos/*.jpg'))\n",
    "    print('len(img_names): ', len(img_names))\n",
    "    img_emb = model.encode(\n",
    "        [Image.open(filepath) for filepath in img_names],\n",
    "        batch_size=128,\n",
    "        convert_to_tensor=True,\n",
    "        show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run clustering algorithm on images\n",
    "# with the threshold parameter, we define at which threshold we identify\n",
    "# two images as similar. Set the threshold lower, and you will get larger clusters which have\n",
    "# less similar images in it (e.g. black cat images vs. cat images vs. animal images).\n",
    "# with min_community_size, we define that we only want to have clusters of a certain minimal size\n",
    "\n",
    "duplicates = util.paraphrase_mining_embeddings(img_emb)\n",
    "\n",
    "# duplicates contains a list with triplets (score, image_id1, image_id2) and is sorted in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the top 10 most similar images in the dataset. These are identical images, for example, the same\n",
    "# photo was uploaded twice to unsplash\n",
    "\n",
    "for score, idx1, idx2 in duplicates[0:10]:\n",
    "    print('\\nScore: {:.3f}'.format(score))\n",
    "    print(img_names[idx1])\n",
    "    display(IPImage(os.path.join(img_folder, img_names[idx1]), width=200))\n",
    "    print(img_names[idx2])\n",
    "    display(IPImage(os.path.join(img_folder, img_names[idx2]), width=200))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5385bbf1daacd4fecc907306b591d855629299b6e8bdef553b3f42c16ecb0317"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
