{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.17k/1.17k [00:00<00:00, 998kB/s]\n",
      "Downloading: 100%|██████████| 850M/850M [01:33<00:00, 9.55MB/s] \n",
      "Downloading: 100%|██████████| 773k/773k [00:02<00:00, 371kB/s]  \n",
      "Downloading: 100%|██████████| 1.32M/1.32M [00:02<00:00, 496kB/s] \n",
      "05/05/2022 10:58:59 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyTextToText\n",
    "\n",
    "# t5 is a text-to-text model: given text it generates text based on the input\n",
    "happy_tt = HappyTextToText(\"T5\", \"t5-base\") # model type and model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.69kB [00:00, 2.72MB/s]                   \n",
      "Downloading metadata: 4.30kB [00:00, 2.54MB/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset jfleg/default (download: 713.98 KiB, generated: 741.90 KiB, post-processed: Unknown size, total: 1.42 MiB) to /Users/yevgeniy_simonov/.cache/huggingface/datasets/jfleg/default/1.0.0/ed4ab2367351fe31949f48849ae6732b164f0d5ea6bb5d4357ff4293ac89511b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 72.7kB [00:00, 15.1MB/s]                   \n",
      "Downloading data: 73.2kB [00:00, 20.2MB/s]                   .48it/s]\n",
      "Downloading data: 73.1kB [00:00, 27.7MB/s]                   .90it/s]\n",
      "Downloading data: 73.4kB [00:00, 1.33MB/s]                   .13it/s]\n",
      "Downloading data: 73.2kB [00:00, 14.8MB/s]                   .83it/s]\n",
      "Downloading data files: 100%|██████████| 5/5 [00:02<00:00,  2.00it/s]\n",
      "Extracting data files: 100%|██████████| 5/5 [00:00<00:00, 1128.90it/s]\n",
      "Downloading data: 72.7kB [00:00, 14.1MB/s]                   \n",
      "Downloading data: 73.1kB [00:00, 14.6MB/s]                   .50it/s]\n",
      "Downloading data: 73.3kB [00:00, 15.0MB/s]                   .95it/s]\n",
      "Downloading data: 73.0kB [00:00, 12.8MB/s]                   .56it/s]\n",
      "Downloading data: 73.4kB [00:00, 23.8MB/s]                   .71it/s]\n",
      "Downloading data files: 100%|██████████| 5/5 [00:02<00:00,  1.82it/s]\n",
      "Extracting data files: 100%|██████████| 5/5 [00:00<00:00, 1683.38it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset jfleg downloaded and prepared to /Users/yevgeniy_simonov/.cache/huggingface/datasets/jfleg/default/1.0.0/ed4ab2367351fe31949f48849ae6732b164f0d5ea6bb5d4357ff4293ac89511b. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"jfleg\", split='validation[:]')\n",
    "eval_dataset = load_dataset(\"jfleg\", split='test[:]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So I think we would not be alive if our ancestors did not develop sciences and technologies . ', 'So I think we could not live if older people did not develop science and technologies . ', 'So I think we can not live if old people could not find science and technologies and they did not develop . ', 'So I think we can not live if old people can not find the science and technology that has not been developed . ']\n",
      "So I think we would not be alive if our ancestors did not develop sciences and technologies . \n",
      "-----------------------------------------------------\n",
      "['Not for use with a car . ', 'Do not use in the car . ', 'Car not for use . ', 'Can not use the car . ']\n",
      "Not for use with a car . \n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# examine dataset\n",
    "for case in train_dataset[\"corrections\"][:2]:\n",
    "    print(case)\n",
    "    print(case[0])\n",
    "    print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import csv\n",
    "def generate_csv(csv_path, dataset):\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow([\"input\", \"target\"])\n",
    "        for case in dataset:\n",
    "            input_text = \"grammar: \" + case[\"sentence\"]\n",
    "            for correction in case[\"corrections\"]:\n",
    "                if input_text and correction:\n",
    "                    writer.writerow([input_text, correction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_csv(\"train.csv\", train_dataset)\n",
    "generate_csv(\"eval.csv\", eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/05/2022 11:44:51 - INFO - happytransformer.happy_transformer -   Preprocessing training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/yevgeniy_simonov/.cache/huggingface/datasets/csv/default-89da93f9a459780b/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 416.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/yevgeniy_simonov/.cache/huggingface/datasets/csv/default-89da93f9a459780b/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 723.41it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 40.42ba/s]\n",
      "05/05/2022 11:44:53 - INFO - happytransformer.happy_transformer -   Training...\n",
      "/Users/yevgeniy_simonov/Documents/Code/Feedmee-app/deep_learning/venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3016\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1131\n",
      " 44%|████▍     | 500/1131 [13:56<19:19,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5804, 'learning_rate': 2.7895667550839967e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1000/1131 [27:59<03:44,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4375, 'learning_rate': 5.7913351016799295e-06, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [31:37<00:00,  1.64s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1131/1131 [31:37<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1897.9385, 'train_samples_per_second': 4.767, 'train_steps_per_second': 0.596, 'train_loss': 0.4986678577966125, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from happytransformer import TTTrainArgs\n",
    "\n",
    "args = TTTrainArgs(batch_size=8)\n",
    "happy_tt.train(\"train.csv\", args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/05/2022 12:16:31 - INFO - happytransformer.happy_transformer -   Preprocessing evaluating data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/yevgeniy_simonov/.cache/huggingface/datasets/csv/default-8aed52ffb85829d2/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2325.00it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 224.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/yevgeniy_simonov/.cache/huggingface/datasets/csv/default-8aed52ffb85829d2/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 592.33it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 29.17ba/s]\n",
      "PyTorch: setting up devices\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2988\n",
      "  Batch size = 1\n",
      "100%|██████████| 2988/2988 [05:56<00:00,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loss:  0.448702335357666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get model loss\n",
    "before_loss = happy_tt.eval(\"eval.csv\")\n",
    "\n",
    "print(\"After loss: \", before_loss.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from happytransformer import TTSettings\n",
    "\n",
    "beam_settings = TTSettings(num_beams=5, min_length=1, max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence has bad grammar and spelling!\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "string = \"grammar: This sentences, has bads grammar and spelling!\"\n",
    "result = happy_tt.generate_text(string, args=beam_settings)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy writing articles on AI.\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "string = \"grammar: I am enjoys, writtings articles ons AI.\"\n",
    "result = happy_tt.generate_text(string, args=beam_settings)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is a god day!\n"
     ]
    }
   ],
   "source": [
    "# Example 3\n",
    "string = \"grammar: wht a god day today!\"\n",
    "result = happy_tt.generate_text(string, args=beam_settings)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in transformer/spelling/config.json\n",
      "Model weights saved in transformer/spelling/pytorch_model.bin\n",
      "tokenizer config file saved in transformer/spelling/tokenizer_config.json\n",
      "Special tokens file saved in transformer/spelling/special_tokens_map.json\n",
      "Copy vocab file to transformer/spelling/spiece.model\n"
     ]
    }
   ],
   "source": [
    "happy_tt.save(\"transformer/spelling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wht a gid day today!    ----->    What a great day today!\n",
      "tday is a nace weather.    ----->    Today is a pleasant weather.\n",
      "i lik prgrammg challengs    ----->    I like programming challengs .\n",
      "is has ben windy todya    ----->    It has been windy todyas .\n",
      "ban2ns, apples, organges agre all tasty fruits    ----->    Bananas, apples, organges and all tasty fruits .\n",
      "i went for a wlk and realsed i forgot to turn off kettl    ----->    I went for a walk and realized I forgot to turn off the lights .\n",
      "intellgnce is a trate of human hind    ----->    Intelligence is a trait of human hindsight .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Examples\n",
    "prefix = \"grammar: \"\n",
    "strings = [\n",
    "    \"wht a gid day today!\",\n",
    "    \"tday is a nace weather.\",\n",
    "    \"i lik prgrammg challengs\",\n",
    "    \"is has ben windy todya\",\n",
    "    \"ban2ns, apples, organges agre all tasty fruits\",\n",
    "    \"i went for a wlk and realsed i forgot to turn off kettl\",\n",
    "    \"intellgnce is a trate of human hind\"\n",
    "]\n",
    "\n",
    "for string in strings:\n",
    "    string_with_prefix = prefix + string\n",
    "    result = happy_tt.generate_text(string_with_prefix, args=beam_settings)\n",
    "    print(string, \"   ----->   \", result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [\n",
    "  (\" .\", \".\"), \n",
    "  (\" ,\", \",\"),\n",
    "  (\" '\", \"'\"),\n",
    "  (\" ?\", \"?\"),\n",
    "  (\" !\", \"!\"),\n",
    "  (\" :\", \"!\"),\n",
    "  (\" ;\", \"!\"),\n",
    "  (\" n't\", \"n't\"),\n",
    "  (\" v\", \"n't\"),\n",
    "  (\"2 0 0 6\", \"2006\"),\n",
    "  (\"5 5\", \"55\"),\n",
    "  (\"4 0 0\", \"400\"),\n",
    "  (\"1 7-5 0\", \"1750\"),\n",
    "  (\"2 0 %\", \"20%\"),\n",
    "  (\"5 0\", \"50\"),\n",
    "  (\"1 2\", \"12\"),\n",
    "  (\"1 0\", \"10\"),\n",
    "  ('\" ballast water', '\"ballast water')\n",
    "]\n",
    "\n",
    "def remove_excess_spaces(text):\n",
    "    for rep in replacements:\n",
    "        text = text.replace(rep[0], rep[1])\n",
    "    return text\n",
    "\n",
    "def generate_csv(csv_path, dataset):\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow([\"input\", \"target\"])\n",
    "        for case in dataset:\n",
    "            input_text = \"grammar: \" + case[\"sentence\"]\n",
    "            for correction in case[\"corrections\"]:\n",
    "                if input_text and correction:\n",
    "                    input_text = remove_excess_spaces(input_text)\n",
    "                    correction = remove_excess_spaces(correction)\n",
    "                    writer.writerow([input_text, correction])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5385bbf1daacd4fecc907306b591d855629299b6e8bdef553b3f42c16ecb0317"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
